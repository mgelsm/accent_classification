{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow libraries\n",
    "import tensorflow as tf\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def silence_remover(sig, fs, threshold):\n",
    "    frame_length = int(0.02 * fs)\n",
    "    nb_frames = len(sig) / frame_length\n",
    "    average_nrj = np.sum(sig**2)/nb_frames\n",
    "    print(average_nrj)\n",
    "    cut_cursor = 0\n",
    "    while np.sum(sig[cut_cursor:cut_cursor+frame_length]**2) < threshold*average_nrj:\n",
    "        cut_cursor += frame_length\n",
    "    return sig[cut_cursor:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.Series(total_label)\n",
    "OneH_labels = pd.get_dummies(labels).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_shf, X_train_shf, y_test_shf, y_train_shf = get_train_test_set(New_data, OneH_labels, perc_for_test=0.2)\n",
    "X_train_shf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct tensorflow graph\n",
    "d = X_train_shf.shape[1]  # data dimensionality\n",
    "nc = 15                 # number of classes\n",
    "Learning_rate = 0.1\n",
    "tf.reset_default_graph()\n",
    "# CG inputs\n",
    "xin = tf.placeholder(tf.float32,[None,d]); print('xin=',xin.get_shape()) # Set the batch size to None such that any batch size can be used in the \n",
    "y_label = tf.placeholder(tf.int32,[None, nc]); # neural net\n",
    "#y_oh = tf.one_hot(y_label, nc, 1.0, 0.0); print('y_oh=',y_oh,y_oh.get_shape())\n",
    "\n",
    "# Fully Connected layer 1\n",
    "with tf.name_scope('FC1'):\n",
    "    W = tf.Variable(tf.truncated_normal([d,2*nc], stddev=0.1 )); #print('W=',W.get_shape())\n",
    "    b = tf.Variable(tf.zeros([2*nc])); #print('b=',b.get_shape())\n",
    "    y1 = tf.matmul(xin, W) + b; #print('y1=',y,y.get_shape())\n",
    "    \n",
    "    tf.summary.histogram(\"layer1/weights\", W)\n",
    "    tf.summary.histogram(\"layer1/biases\", b)\n",
    "\n",
    "# ReLu activation\n",
    "y2 = tf.nn.relu(y1)\n",
    "tf.summary.histogram(\"relu_activation\", y2)\n",
    "\n",
    "# Fully Connected layer 2\n",
    "with tf.name_scope('FC2'):\n",
    "    W2 = tf.Variable(tf.truncated_normal([2*nc,nc], stddev=0.1 )); #print('W=',W.get_shape())\n",
    "    b2 = tf.Variable(tf.zeros([nc]));\n",
    "    y = tf.matmul(y2, W2) + b2; \n",
    "\n",
    "    tf.summary.histogram(\"layer2/weights\", W2)\n",
    "    tf.summary.histogram(\"layer2/biases\", b2)\n",
    "\n",
    "# Softmax\n",
    "y = tf.nn.softmax(y); print('y3=',y,y.get_shape())\n",
    "tf.summary.histogram(\"softmax_activation\", y)\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    # L2 Regularization\n",
    "    reg_loss = 0.0\n",
    "    reg_loss += tf.nn.l2_loss(W)\n",
    "    reg_loss += tf.nn.l2_loss(b)\n",
    "    reg_loss += tf.nn.l2_loss(W2)\n",
    "    reg_loss += tf.nn.l2_loss(b2)\n",
    "    reg_par = 1*1e-3\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y))\n",
    "    # Loss\n",
    "    total_loss = cross_entropy #+ reg_par* reg_loss\n",
    "    tf.summary.scalar(\"total_loss\", total_loss)\n",
    "    \n",
    "# Optimization scheme\n",
    "train_step = tf.train.AdamOptimizer(Learning_rate).minimize(total_loss)\n",
    "\n",
    "y_out = y\n",
    "# Accuracy\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_label,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "summ = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Graph\n",
    "n = X_train_shf.shape[0]\n",
    "indices = collections.deque()\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "test_writer = tf.summary.FileWriter(\"tensorboard/mlp/test\")\n",
    "train_writer = tf.summary.FileWriter(\"tensorboard/mlp/train\")\n",
    "train_writer.add_graph(sess.graph)\n",
    "sess.run(init)\n",
    "batch_size = 100\n",
    "for i in range(10001):\n",
    "    \n",
    "    # Batch extraction\n",
    "    if len(indices) < batch_size:\n",
    "        indices.extend(np.random.permutation(n)) \n",
    "    idx = [indices.popleft() for i in range(batch_size)]\n",
    "    batch_x, batch_y = X_train_shf[idx], y_train_shf[idx]\n",
    "    #print(batch_x.shape,batch_y.shape)\n",
    "    \n",
    "    # save accuracies\n",
    "    if not i%1000:\n",
    "        yo, loss, summary_tr, acc_train = sess.run([y_out,total_loss, summ, accuracy], feed_dict={xin: batch_x, y_label: batch_y})\n",
    "        train_writer.add_summary(summary_tr, i)\n",
    "        print('\\nIteration i=',i,', train accuracy=',acc_train,', loss=',loss)\n",
    "        #print('\\nIteration i=',i,', train accuracy=',acc_train)\n",
    "        summary_t, acc_test = sess.run([summ, accuracy], feed_dict={xin: X_test_shf, y_label: y_test_shf})\n",
    "        test_writer.add_summary(summary_t, i)\n",
    "        print('test accuracy=',acc_test)\n",
    "    \n",
    "    else: # train mlp    \n",
    "        train_accuracy = sess.run([accuracy], feed_dict={xin: batch_x, y_label: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into traing and testing set\n",
    "\n",
    "X_test_shf, X_train_shf, y_test_shf, y_train_shf = get_train_test_set(X_c, labels, perc_for_test=0.2)\n",
    "print('train size : ' + str(X_train_shf.shape))\n",
    "print('test size : ' + str(X_test_shf.shape))\n",
    "X_train_shf = X_train_shf.astype(float)\n",
    "X_train_shf = X_train_shf.astype(float)\n",
    "y_train_shf = y_train_shf.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define computational graph (CG)\n",
    "d = X_train_shf.shape[1]  # number of frames\n",
    "nb_ch = 13 # number of channels\n",
    "nc = 15                  # number of classes\n",
    "Learning_rate = 0.001\n",
    "batch_size = X_test_shf.shape[0]\n",
    "\n",
    "# tensor inputs\n",
    "tf.reset_default_graph()\n",
    "xin = tf.placeholder(tf.float32,[batch_size,d,nb_ch]); print('xin=',xin,xin.get_shape()) # allow any size of datapoints\n",
    "y_label = tf.placeholder(tf.int32,[batch_size]); #print('y_label=',y_label,y_label.get_shape())\n",
    "y_oh = tf.one_hot(y_label, nc); #print('y_oh =',y_oh,y_oh.get_shape())\n",
    "\n",
    "\n",
    "# Convolutional layer\n",
    "F_size = 50   # size of the filter\n",
    "nb_F = 30  # number of filters\n",
    "ncl = K*K*F\n",
    "with tf.name_scope('layer1'):\n",
    "    Wcl = tf.Variable(tf.truncated_normal([F_size,nb_ch,nb_F], stddev=tf.sqrt(2./tf.to_float(ncl)) )); #print('Wcl=',Wcl.get_shape())\n",
    "    bcl = tf.Variable(tf.zeros([F])); #print('bcl=',bcl.get_shape())\n",
    "    x = tf.nn.conv1d(xin, Wcl, stride=1 , padding='SAME')\n",
    "    x += bcl; print('x2=',x.get_shape())\n",
    "\n",
    "    tf.summary.histogram(\"layer1/weights\", Wcl)\n",
    "    tf.summary.histogram(\"layer1/biases\", bcl)\n",
    "\n",
    "# batch normalization\n",
    "#x = tf.contrib.layers.batch_norm(x)\n",
    "\n",
    "# dropout\n",
    "#keep_prob = tf.placeholder(tf.float32)\n",
    "#tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "#x = tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "# ReLU activation\n",
    "with tf.name_scope('Relu_activation'):\n",
    "    x = tf.nn.relu(x)\n",
    "    tf.summary.histogram(\"relu_activation\", x)\n",
    "\n",
    "# Fully Connected layer\n",
    "nfc = d*nb_F\n",
    "with tf.name_scope('FC'):\n",
    "    x = tf.reshape(x, [batch_size,-1]); print('x3=',x.get_shape())\n",
    "    Wfc = tf.Variable(tf.truncated_normal([nfc,nc], stddev=tf.sqrt(6./tf.to_float(nfc+nc)) )); print('Wfc=',Wfc.get_shape())\n",
    "    bfc = tf.Variable(tf.zeros([nc])); print('bfc=',bfc.get_shape())\n",
    "    y = tf.matmul(x, Wfc); print('y1=',y,y.get_shape())\n",
    "    y += bfc; print('y2=',y,y.get_shape())\n",
    "\n",
    "# Softmax\n",
    "with tf.name_scope('softmax_activation'):\n",
    "    y = tf.nn.softmax(y); print('y3=',y,y.get_shape())\n",
    "\n",
    "# Loss\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_oh * tf.log(y), 1))\n",
    "    total_loss = cross_entropy\n",
    "    tf.summary.scalar(\"total_loss\", total_loss)\n",
    "print('cross_entropy=',cross_entropy,cross_entropy.get_shape())\n",
    "\n",
    "# Optimization scheme\n",
    "#train_step = tf.train.GradientDescentOptimizer(0.025).minimize(total_loss)\n",
    "train_step = tf.train.AdamOptimizer(Learning_rate).minimize(total_loss)\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_oh,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "summ = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Computational Graph\n",
    "n = X_train_shf.shape[0]\n",
    "indices = collections.deque()\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('tensorboard/cnn/train',sess.graph)\n",
    "test_writer = tf.summary.FileWriter('tensorboard/cnn/test')\n",
    "\n",
    "sess.run(init)\n",
    "for i in range(10001):\n",
    "    \n",
    "    # Batch extraction\n",
    "    if len(indices) < batch_size:\n",
    "        indices.extend(np.random.permutation(n)) \n",
    "    idx = [indices.popleft() for i in range(batch_size)]\n",
    "    batch_x, batch_y = X_train_shf[idx], y_train_shf[idx]\n",
    "    #print(batch_x.shape,batch_y.shape)\n",
    "    \n",
    "    # Run CG for variable training\n",
    "    summary, _, acc_train, total_loss_o = sess.run([summ, train_step,accuracy,total_loss], feed_dict={xin: batch_x, y_label: batch_y})\n",
    "    train_writer.add_summary(summary, i)\n",
    "    # Run CG for test set\n",
    "    if not i%1000:\n",
    "        print('\\nIteration i=',i,', train accuracy=',acc_train,', loss=',total_loss_o)\n",
    "        acc_test = sess.run(accuracy, feed_dict={xin: X_test_shf, y_label: y_test_shf})\n",
    "        print('test accuracy=',acc_test)\n",
    "    if i % 10 == 0:  # Record summaries and test-set accuracy\n",
    "        acc = sess.run([accuracy], feed_dict={xin: X_test_shf, y_label: y_test_shf})\n",
    "        test_writer.add_summary(summary_t, i)\n",
    "        #print('Accuracy at step %s: %s' % (i, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "# 3D visualization\n",
    "import pylab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "\n",
    "Cgt = labels\n",
    "n = dist_matrix.shape[0]\n",
    "\n",
    "W = scipy.sparse.csc.csc_matrix(dist_matrix)\n",
    "d = W.sum(axis=0)\n",
    "d += np.spacing(np.array(0, W.dtype)) # d += epsilon\n",
    "d = 1.0 / np.sqrt(d)\n",
    "D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "I = scipy.sparse.identity(d.size, dtype=W.dtype)\n",
    "L = I - D * W * D\n",
    "\n",
    "# Compute K smallest eigenvectors/eigenvalues of A\n",
    "lamb, U = scipy.sparse.linalg.eigsh(L, k=4, which='SM')\n",
    "\n",
    "# Sort eigenvalue from smallest to largest values\n",
    "idx = lamb.argsort() # increasing order\n",
    "lamb, U = lamb[idx], U[:,idx]\n",
    "print(lamb)\n",
    "\n",
    "# Y*\n",
    "Y = U\n",
    "\n",
    "# Plot in 3D the 2nd, 3rd, 4th columns of Y*\n",
    "Xdisp = Y[:,1]\n",
    "Ydisp = Y[:,2]\n",
    "Zdisp = Y[:,3]\n",
    "\n",
    "# 2D Visualization\n",
    "plt.figure(14)\n",
    "size_vertex_plot = 10\n",
    "plt.scatter(Xdisp, Ydisp, s=size_vertex_plot*np.ones(n), c=Cgt)\n",
    "plt.title('2D Visualization')\n",
    "plt.show()\n",
    "\n",
    "# 3D Visualization\n",
    "fig = pylab.figure(15)\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(Xdisp, Ydisp, Zdisp, c=Cgt)\n",
    "pylab.title('3D Visualization')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.specgram(sig, NFFT=512, Fs=fs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
